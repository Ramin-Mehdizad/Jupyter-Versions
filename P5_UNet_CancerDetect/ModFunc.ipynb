{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# Created on Feb 30, 2020\n",
    "#\n",
    "# @author: Ramin Mehdizad Tekiyeh\n",
    "#\n",
    "# This code is written in Python 3.7.4 , Spyder 3.3.6\n",
    "#===============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# imports\n",
    "#==============================================================================\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread, imshow\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras.backend as k\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "import keras\n",
    "import argparse\n",
    "import sys\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# import modules\n",
    "#==============================================================================\n",
    "import ModVar as Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function asks user input data\n",
    "#==============================================================================  \n",
    "def Input_Data_Message():\n",
    "    \n",
    "    print('')\n",
    "    print('|===========================================================')\n",
    "    print('|  ==> To run the code with default values, just press Enter')\n",
    "    print('|  ==> Otherwise:')\n",
    "    print('|  ==> Enter the parameters as following format:')\n",
    "    print('|')\n",
    "    print('|  -e 2 -b 2 -f 0.1 -p 5 -y 64 -x 64 -z 3 -m 0 -s 1 -l 1')\n",
    "    print('|')\n",
    "    print('|  ==> To get help, type \"-h\" and press Enter')\n",
    "    print('|  ==> To exit program, type \"Q\" and press Enter')\n",
    "    print('|===========================================================')\n",
    "    \n",
    "    Var.str_input=input('  Enter parameters: ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function asks user input data\n",
    "#==============================================================================\n",
    "def Call_Parser():\n",
    "    \n",
    "    # create parse class\n",
    "    parser1 = argparse.ArgumentParser(add_help=True,prog='U_Net Cancer Detection',\n",
    "             description='* This program detects cancer regions in medical pictures *')\n",
    "    \n",
    "    # set program version\n",
    "    parser1.add_argument('-v','--version',action='version',\n",
    "                        version='%(prog)s 1.0')\n",
    "\n",
    "    # number of Epochs\n",
    "    parser1.add_argument('-e', '--NumEpochs', action='store', \n",
    "                         default='50',  dest='NumEpochs',\n",
    "                         help='define number of epochs (integer number greater than 0')\n",
    "                         \n",
    "    # number of batch size\n",
    "    parser1.add_argument('-b', '--BatchSize', action='store', \n",
    "                         default='2',  dest='BatchSize',\n",
    "                         help='define batch_size')\n",
    "                         \n",
    "    # validation fraction\n",
    "    parser1.add_argument('-f', '--ValFrac', action='store', \n",
    "                         default='0.1',  dest='ValidationFraction',\n",
    "                         help='input percent of train data to be used in validation')\n",
    "    \n",
    "    # number of patience epochs to stop process\n",
    "    parser1.add_argument('-p', '--NumPatience', action='store', \n",
    "                         default='5',  dest='NumPatience',\n",
    "                         help='the number of epochs as patience to stop the process')\n",
    "    \n",
    "    # height of the pictures to be used in the model\n",
    "    parser1.add_argument('-y', '--Height', action='store', \n",
    "                         default='256',  dest='Height',\n",
    "                         help='Height of the pictures to be used in the model')\n",
    "    \n",
    "    # width of the pictures to be used in the model\n",
    "    parser1.add_argument('-x', '--Width', action='store', \n",
    "                         default='256',  dest='Width',\n",
    "                         help='Width of the pictures to be used in the model')\n",
    "    \n",
    "    # number of channels of the pictures \n",
    "    parser1.add_argument('-z', '--Channel', action='store', \n",
    "                         default='3',  dest='Channel',\n",
    "                         help='number of channels of the train and test pictures')\n",
    "    \n",
    "    # create movie from predicted masks in each epoch\n",
    "    parser1.add_argument('-m', '--EpochMovie', action='store', \n",
    "                         default='0',  dest='EpochMovie', choices=['0', '1'],\n",
    "                         help='0: Dont create movie     1: create movie')\n",
    "    \n",
    "    # whether to save plots or not\n",
    "    parser1.add_argument('-s', '--SavePlot', action='store', \n",
    "                         default='1',  dest='SavePlot', choices=['0', '1'],\n",
    "                         help='0: Dont Save plots     1: Save plots')\n",
    "    \n",
    "    # whether to create log file or not\n",
    "    parser1.add_argument('-l', '--log', action='store',\n",
    "                         default='1', dest='logFile', choices=['0', '1'],\n",
    "                         help='0: Dont write logfile     1: write logfile')\n",
    "    \n",
    "    # indicates when to exit while loop\n",
    "    entry=False\n",
    "    while entry==False:\n",
    "        \n",
    "        # initialize\n",
    "        ParsErr=0\n",
    "        \n",
    "        # --------------in this section we try to parse successfully-----------\n",
    "        \n",
    "        # function to call input message from command line    \n",
    "        Input_Data_Message()\n",
    "        \n",
    "        # user wanted to continue with default values\n",
    "        if Var.str_input=='':\n",
    "            Var.args=parser1.parse_args()\n",
    "            # so we exit while loop\n",
    "            entry=True\n",
    "            ParsErr=0\n",
    "        elif Var.str_input.upper()=='Q':\n",
    "            # exit script\n",
    "            sys.exit()\n",
    "        else:\n",
    "            entry=True\n",
    "            ParsErr=0\n",
    "            try:\n",
    "                Var.args=parser1.parse_args(Var.str_input.split(' '))\n",
    "                print(Var.args)\n",
    "            except:\n",
    "                entry=False\n",
    "                ParsErr=1\n",
    "                print('--- Error: entered data not correct ---\\n')\n",
    "        #----------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        #-------------After having parsed successfully, we coninue-------------\n",
    "        # continue if parse was done successfully\n",
    "        if ParsErr==0:  \n",
    "            if os.path.isdir('./dataset'):\n",
    "                print('dataset path exists')\n",
    "            else:\n",
    "                print(\"--- Dataset address doesn't exist ---\")\n",
    "                print('Make sur ou have \"Dataset\" folder in your python code path.')\n",
    "                entry=False\n",
    "        #----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function prinrts parsed data\n",
    "#==============================================================================\n",
    "def PrintParsedData(): \n",
    "    print('') \n",
    "    print('  =====================Parsed  Data==================')  \n",
    "    print('')\n",
    "    print('  number of epoch       =', Var.args.NumEpochs)\n",
    "    print('  batch size            =', Var.args.BatchSize)\n",
    "    print('  validation fraction   =', Var.args.ValidationFraction)\n",
    "    print('  stop patience         =', Var.args.NumPatience)\n",
    "    print('  picture height        =', Var.args.Height)\n",
    "    print('  picture width         =', Var.args.Width)\n",
    "    print('  number of channels    =', Var.args.Channel)\n",
    "    print('  save plots            =', Var.args.SavePlot)\n",
    "    print('  Create Log File       =', Var.args.logFile)\n",
    "    print('  ===================================================')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function gets computer spec\n",
    "#==============================================================================\n",
    "def GetSysInfo():\n",
    "    import platform,socket,re,uuid,psutil\n",
    "    try:\n",
    "        Var.sysinfo.append(['platform',platform.system()]) \n",
    "        Var.sysinfo.append(['platform-release',platform.release()])\n",
    "        Var.sysinfo.append(['platform-version',platform.version()])\n",
    "        Var.sysinfo.append(['architecture',platform.machine()])\n",
    "        Var.sysinfo.append(['hostname',socket.gethostname()])\n",
    "        Var.sysinfo.append(['ip-address',socket.gethostbyname(socket.gethostname())])\n",
    "        Var.sysinfo.append(['mac-address',':'.join(re.findall('..', '%012x' % uuid.getnode()))])\n",
    "        Var.sysinfo.append(['processor',platform.processor()])\n",
    "        Var.sysinfo.append(['ram',str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function defines train and test path\n",
    "#==============================================================================\n",
    "def Define_Train_Test_Path():\n",
    "    try:\n",
    "        Var.Train_Image_Path = './dataset/Train_Pics'\n",
    "        Var.Train_Mask_Path = './dataset/Train_Masks'\n",
    "        Var.Test_Image_Path = './dataset/Test_Pics'\n",
    "        Var.Test_Mask_Path = './dataset/Test_Masks'\n",
    "    except:\n",
    "        print('---Train and test data path error---')\n",
    "        print('---make sure train and test data are i the dataset folder---')\n",
    "        # exit script\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function preprocesses train data\n",
    "#==============================================================================\n",
    "def PreProc_Train():\n",
    "    \n",
    "    NotResized_Image = np.zeros((len(Var.Train_Mask_List), 768, 896, 3), dtype = np.uint8)\n",
    "    NotResized_Mask = np.zeros((len(Var.Train_Mask_List), 768, 896), dtype = np.bool)\n",
    "    Train_X = np.zeros((len(Var.Train_Mask_List), Var.Height, Var.Width, Var.Channels), dtype = np.uint8)\n",
    "    Train_Y = np.zeros((len(Var.Train_Mask_List), Var.Height, Var.Width, 1), dtype = np.bool)\n",
    "    \n",
    "    n = 0\n",
    "    for mask_path in glob.glob('{}/*.TIF'.format(Var.Train_Mask_Path)):\n",
    "        \n",
    "        base = os.path.basename(mask_path)\n",
    "        image_ID, ext = os.path.splitext(base)\n",
    "        image_path = '{}/{}_ccd.tif'.format(Var.Train_Image_Path, image_ID)\n",
    "        Source_Image = imread(image_path)\n",
    "        Source_Mask = imread(mask_path)\n",
    "        \n",
    "        y_coord, x_coord = np.where(Source_Mask == 255)\n",
    "        \n",
    "        y_min = min(y_coord) \n",
    "        y_max = max(y_coord)\n",
    "        x_min = min(x_coord)\n",
    "        x_max = max(x_coord)\n",
    "        \n",
    "        cropped_image = Source_Image[y_min:y_max, x_min:x_max]\n",
    "        cropped_mask = Source_Mask[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        \n",
    "        Train_X[n] = resize(cropped_image[:,:,:Var.Channels],\n",
    "               (Var.Height, Var.Width, Var.Channels),\n",
    "               mode = 'constant',\n",
    "               anti_aliasing=True,\n",
    "               preserve_range=True)\n",
    "        \n",
    "        Train_Y[n] = np.expand_dims(resize(cropped_mask, \n",
    "               (Var.Height, Var.Width),\n",
    "               mode = 'constant',\n",
    "               anti_aliasing=True,\n",
    "               preserve_range=True), axis = -1)\n",
    "        \n",
    "        # not resized versions of pictures are saved for comparison with model predictions\n",
    "        NotResized_Image[n] = Source_Image\n",
    "        NotResized_Mask[n] = Source_Mask\n",
    "        \n",
    "        n+=1\n",
    "        \n",
    "    return Train_X, Train_Y, NotResized_Image, NotResized_Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function preprocesses test data\n",
    "#==============================================================================\n",
    "def PreProc_Test():\n",
    "    \n",
    "    Test_X = np.zeros((len(Var.Test_Mask_List), Var.Height, Var.Width, Var.Channels), dtype = np.uint8)\n",
    "    Test_Y = np.zeros((len(Var.Test_Mask_List), Var.Height, Var.Width, 1), dtype = np.bool)\n",
    "    \n",
    "    n = 0\n",
    "    for mask_path in glob.glob('{}/*.TIF'.format(Var.Test_Mask_Path)):\n",
    "        \n",
    "        base = os.path.basename(mask_path)\n",
    "        image_ID, ext = os.path.splitext(base)\n",
    "        image_path = '{}/{}_ccd.tif'.format(Var.Test_Image_Path, image_ID)\n",
    "        Source_Image = imread(image_path)\n",
    "        Source_Mask = imread(mask_path)\n",
    "        \n",
    "        y_coord, x_coord = np.where(Source_Mask == 255)\n",
    "        \n",
    "        y_min = min(y_coord) \n",
    "        y_max = max(y_coord)\n",
    "        x_min = min(x_coord)\n",
    "        x_max = max(x_coord)\n",
    "        \n",
    "        cropped_image = Source_Image[y_min:y_max, x_min:x_max]\n",
    "        cropped_mask = Source_Mask[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        Test_X[n] = resize(cropped_image[:,:,:Var.Channels],\n",
    "               (Var.Height, Var.Width, Var.Channels),\n",
    "               mode = 'constant',\n",
    "               anti_aliasing=True,\n",
    "               preserve_range=True)\n",
    "        \n",
    "        # here we add 1 dmension at the end: axis=-1  so that tensor dimension\n",
    "        # is equal to dimension of main pictures. in fact one-hot-encoding is \n",
    "        # pplied in this manner\n",
    "        Test_Y[n] = np.expand_dims(resize(cropped_mask, \n",
    "               (Var.Height, Var.Width),\n",
    "               mode = 'constant',\n",
    "               anti_aliasing=True,\n",
    "               preserve_range=True), axis = -1)\n",
    "        \n",
    "        n+=1\n",
    "        \n",
    "    return Test_X, Test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function Extract List of train and test Picturs Addresses\n",
    "#==============================================================================\n",
    "def Extract_List_of_Picture_Addresses():\n",
    "    Var.Train_Mask_List = sorted(next(os.walk(Var.Train_Mask_Path))[2])\n",
    "    Var.Test_Mask_List = sorted(next(os.walk(Var.Test_Mask_Path))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function sets U_Net model parameters\n",
    "#==============================================================================\n",
    "def Set_Model_Param():\n",
    "    \n",
    "    # desired Images and Mask Sizes to be used in UNet\n",
    "    Var.Height=int(Var.args.Height)\n",
    "    Var.Width=int(Var.args.Width)\n",
    "    Var.Channels=int(Var.args.Channel)\n",
    "    Var.input_size=(Var.Height, Var.Width, Var.Channels)\n",
    "    \n",
    "    # modelparameters\n",
    "    Var.n_epochs=int(Var.args.NumEpochs)\n",
    "    Var.n_batch_size=int(Var.args.BatchSize)\n",
    "    Var.n_patience=int(Var.args.NumPatience)\n",
    "    Var.validation_split=float(Var.args.ValidationFraction)\n",
    "\n",
    "    Var.imageset = 'BCC'\n",
    "    Var.backbone = 'UNET'\n",
    "    Var.version = 'v1.0'\n",
    "    Var.model_h5 = 'model-{imageset}-{backbone}-{version}.h5'.format(imageset=Var.imageset, \n",
    "                      backbone = Var.backbone, version = Var.version)\n",
    "    Var.model_h5_checkpoint = '{model_h5}.checkpoint'.format(model_h5=Var.model_h5)\n",
    "    Var.earlystopper = EarlyStopping(patience=Var.n_patience, verbose=1)\n",
    "    Var.checkpointer = ModelCheckpoint(Var.model_h5_checkpoint, verbose = 1, save_best_only=True)\n",
    "    Var.Each_Epoch_Predict=loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Define U_NET Model Evaluator (Intersection Over Union _ IOU)\n",
    "#==============================================================================\n",
    "def Mean_IOU_Evaluator(y_true, y_pred):\n",
    "    \n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1, 0.05):\n",
    "        \n",
    "        y_pred_ = tf.to_int32(y_pred>t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        k.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return k.mean(k.stack(prec), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function plots preprocessing results\n",
    "#==============================================================================\n",
    "def Plot_PreProc_Results():\n",
    "    \n",
    "    print('**************** Results of preprocessing ****************')\n",
    "    \n",
    "    n_pic=5   \n",
    "     \n",
    "    imshow(Var.NotResized_Image[n_pic])\n",
    "    plt.title('Original_Image')\n",
    "    plt.show()\n",
    "    \n",
    "    imshow(Var.NotResized_Mask[n_pic])\n",
    "    plt.title('Original_Mask')\n",
    "    plt.show()\n",
    "    \n",
    "    imshow(Var.Train_Images[n_pic])\n",
    "    plt.title('Region_of_Interest_Image')\n",
    "    plt.show()\n",
    "    \n",
    "    # we must use np.squeeze because it is a one-hot-encoded picture data\n",
    "    imshow(np.squeeze(Var.Train_Masks[n_pic]))\n",
    "    plt.title('Region_of_Interest_Mask')\n",
    "    plt.show()\n",
    "    \n",
    "    rows, columns = 1,4\n",
    "    Figure = plt.figure(figsize=(5,5))\n",
    "    plt.title('preprocessed pictures')\n",
    "    Image_List = [Var.NotResized_Image[n_pic], Var.NotResized_Mask[n_pic], Var.Train_Images[n_pic], Var.Train_Masks[n_pic]]\n",
    "    \n",
    "    for i in range(0, rows*columns ):\n",
    "        Image = Image_List[i]\n",
    "        Sub_Plot_Image = Figure.add_subplot(rows, columns, i+1)\n",
    "        # since one of the pictures is one-hot-encoded, we use np.sueeze in the code\n",
    "        Sub_Plot_Image.imshow(np.squeeze(Image))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Implementation of U_NET Model \n",
    "#==============================================================================\n",
    "def U_Net_Segmentation():\n",
    "    \n",
    "    inputs = Input(Var.input_size)\n",
    "    n = Lambda(lambda x:x/255)(inputs)\n",
    "    \n",
    "    \n",
    "    c1 = Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(n)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2,2))(c1)\n",
    "\n",
    "\n",
    "    c2 = Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2,2))(c2)\n",
    "\n",
    "\n",
    "    c3 = Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2,2))(c3)\n",
    "\n",
    "\n",
    "    c4 = Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2,2))(c4)\n",
    "\n",
    "\n",
    "    c5 = Conv2D(256, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c6)   \n",
    "\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c7) \n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c8) \n",
    "    \n",
    "    \n",
    "    u9 = Conv2DTranspose(16, (2,2), strides=(2,2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis = 3)\n",
    "    c9 = Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3,3), activation='elu', kernel_initializer='he_normal',\n",
    "                padding='same')(c9) \n",
    "    \n",
    "    outputs = Conv2D(1,(1,1), activation='sigmoid')(c9)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                  metrics=[Mean_IOU_Evaluator])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this class Shows The Results per Epoch \n",
    "#==============================================================================  \n",
    "class loss_history(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__ (self, x=4):\n",
    "        self.x = x\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        \n",
    "        imshow(Var.Train_Images[self.x])\n",
    "        plt.title('train image in this epoch')\n",
    "        plt.show()\n",
    "        \n",
    "        imshow(np.squeeze(Var.Train_Masks[self.x]))\n",
    "        plt.title('train mask in this epoch')\n",
    "        plt.show()\n",
    "        \n",
    "        Var.preds_train = self.model.predict(np.expand_dims(Var.Train_Images[self.x], axis = 0))\n",
    "        imshow(np.squeeze(Var.preds_train[0]))\n",
    "        plt.show()\n",
    "        \n",
    "        Var.Each_Epoch_Predicted_Images.append(np.squeeze(Var.preds_train[0]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function plots the final results \n",
    "#==============================================================================       \n",
    "def Plot_Final_Results():\n",
    "    \n",
    "    print('======================= final results =====================')\n",
    "    \n",
    "    #------ train data predicted plots --------\n",
    "    \n",
    "    # randomly select a data in train pictures\n",
    "    j = random.randint(0, len(Var.Train_Images)-1)\n",
    "    \n",
    "    print('Train_Image')\n",
    "    imshow(Var.Train_Images[j])\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train_Mask')\n",
    "    imshow(np.squeeze(Var.Train_Masks[j]))\n",
    "    plt.show()\n",
    "    \n",
    "    print('Segmented_Image')\n",
    "    imshow(np.squeeze(Var.preds_train[j]))\n",
    "    plt.show()\n",
    "    \n",
    "    # plot results in subplot \n",
    "    rows, columns = 1,3\n",
    "    fig_train_predicted = plt.figure(figsize=(15,5))\n",
    "    plt.title('Train Data No {}Predicted'.format(j))\n",
    "    Image_List = [Var.Train_Images[j], Var.Train_Masks[j],\n",
    "                  Var.preds_train[j]]\n",
    "    \n",
    "    for i in range(0, rows*columns ):\n",
    "        Image = Image_List[i]\n",
    "        Sub_Plot_Image = fig_train_predicted.add_subplot(rows, columns, i+1)\n",
    "        # since one of the pictures is one-hot-encoded, we use np.sueeze in the code\n",
    "        Sub_Plot_Image.imshow(np.squeeze(Image))\n",
    "    plt.show()\n",
    "    if Var.SavePlotFlag: fig_train_predicted.savefig('Train Data Pred',dpi=100)\n",
    "    #-----------------------------------------\n",
    "    \n",
    "        \n",
    "    #------ test data predicted plots --------\n",
    "    \n",
    "    # randomly select a test picture (we have just 2 test picture)\n",
    "    j = random.randint(0,1)\n",
    "    \n",
    "    print('Test_Image')\n",
    "    imshow(Var.Test_Images[j])\n",
    "    plt.show()\n",
    "    \n",
    "    print('Test_Mask')\n",
    "    imshow(np.squeeze(Var.Test_Masks[j]))\n",
    "    plt.show()\n",
    "    \n",
    "    print('Segmented_Test_Mask')\n",
    "    imshow(np.squeeze(Var.preds_test[j]))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # plot results in subplot \n",
    "    rows, columns = 1,3\n",
    "    fig_test_predicted = plt.figure(figsize=(15,5))\n",
    "    plt.title('Test Data No {} Predicted'.format(j))\n",
    "    Image_List = [Var.Test_Images[j], Var.Test_Masks[j],\n",
    "                  Var.preds_test[j]]\n",
    "    for i in range(0, rows*columns ):\n",
    "        Image = Image_List[i]\n",
    "        Sub_Plot_Image = fig_test_predicted.add_subplot(rows, columns, i+1)\n",
    "        # since one of the pictures is one-hot-encoded, we use np.sueeze in the code\n",
    "        Sub_Plot_Image.imshow(np.squeeze(Image))\n",
    "    \n",
    "    plt.show()\n",
    "    if Var.SavePlotFlag: fig_test_predicted.savefig('Test Data Pred',dpi=100)\n",
    "    #-----------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # Show Loss and IOU Plots\n",
    "    fig_Loss=plt.figure()\n",
    "    plt.plot(Var.results.history['loss'])\n",
    "    plt.plot(Var.results.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training','Validation'], loc = 'upper left')\n",
    "    plt.show()\n",
    "    if Var.SavePlotFlag: fig_Loss.savefig('fig_Loss',dpi=100)\n",
    "    \n",
    "    # Summarize History for IOU\n",
    "    fig_IOU=plt.figure()\n",
    "    plt.plot(Var.results.history['Mean_IOU_Evaluator'])\n",
    "    plt.plot(Var.results.history['val_Mean_IOU_Evaluator'])\n",
    "    plt.title('Intersection Over Union')\n",
    "    plt.ylabel('IOU')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training','Validation'], loc = 'upper left')\n",
    "    plt.show() \n",
    "    if Var.SavePlotFlag: fig_IOU.savefig('fig_IOU',dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function evaluates the model by use of test models\n",
    "#==============================================================================\n",
    "def Evaluate_By_Test_Samples():\n",
    "    Var.preds_train = Var.model.predict(Var.Train_Images, verbose=1)\n",
    "    Var.preds_train_t = (Var.preds_train>0.5).astype(np.uint8)\n",
    "    Var.preds_test = Var.model.predict(Var.Test_Images, verbose=1)\n",
    "    Var.preds_test_t = (Var.preds_test>0.5).astype(np.uint8)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# create movie of predicted masks in each epoch\n",
    "#==============================================================================\n",
    "def Create_Movie():\n",
    "\n",
    "    # ------------------------ make animation from epochs----------------------\n",
    "    from matplotlib import animation\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    \n",
    "    # movie predicted masks\n",
    "    \n",
    "    FFMpegWriter = animation.writers['ffmpeg']\n",
    "    \n",
    "    metadata = dict(title='Movie Test', artist='Matplotlib',\n",
    "                    comment='Movie support!')\n",
    "    myWriter = FFMpegWriter(fps=5, metadata=metadata)\n",
    "        \n",
    "    fig3=plt.figure(figsize=(10,10))\n",
    "    with myWriter.saving(fig3, \"Epoch_Movie.mp4\", 150):\n",
    "        for i in range(len(Var.Each_Epoch_Predicted_Images)):\n",
    "            \n",
    "            with plt.style.context('Solarize_Light2'):\n",
    "                plt.clf()\n",
    "                \n",
    "                # plot predicted mask\n",
    "#                imshow(np.squeeze(Var.Each_Epoch_Predicted_Images[i]))\n",
    "                \n",
    "                plt.imshow(Var.Train_Images[0])\n",
    "                \n",
    "                plt.show()\n",
    "          \n",
    "\n",
    "                myWriter.grab_frame() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

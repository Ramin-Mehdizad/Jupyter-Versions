{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# Created on Feb 15, 2020\n",
    "#\n",
    "# @author: Ramin Mehdizad Tekiyeh\n",
    "#\n",
    "# This code is written in Python 3.7.4 , Spyder 3.3.6\n",
    "#==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# importing standard classes\n",
    "#==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import os.path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# importing module codes\n",
    "#==============================================================================\n",
    "import ModVar as Var\n",
    "import ModFunc as Func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function asks user input data\n",
    "#==============================================================================  \n",
    "def Input_Data_Message():\n",
    "    \n",
    "    print('')\n",
    "    print('|===========================================================')\n",
    "    print('|  ==> To run the code with default values, just press Enter')\n",
    "    print('|  ==> Otherwise:')\n",
    "    print('|  ==> Enter the parameters as following format:')\n",
    "    print('|')\n",
    "    print('|  -d D:/.../*.csv -t D:/.../*.csv -w D:/... -p 0 -l 0 -r 1')\n",
    "    print('|')\n",
    "    print('|  ==> To get help, type \"-h\" and press Enter')\n",
    "    print('|  ==> To exit program, type \"Q\" and press Enter')\n",
    "    print('|===========================================================')\n",
    "    \n",
    "    Var.str_input=input('  Enter parameters: ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function asks user input data\n",
    "#==============================================================================\n",
    "def Call_Parser():\n",
    "    \n",
    "    # create parse class\n",
    "    parser1 = argparse.ArgumentParser(add_help=True,prog='Music_Hit Program',\n",
    "             description='* This program analyzes Music_Hit by use of\\\n",
    "                 several classifiers*')\n",
    "    \n",
    "    # set program version\n",
    "    parser1.add_argument('-v','--version',action='version',\n",
    "                        version='%(prog)s 1.0')\n",
    "\n",
    "    parser1.add_argument('-f', '--ScriptPath', action='store',\n",
    "                        default=Var.MainDir,\n",
    "                        dest='ScriptPath',help='Shows Script File Address')\n",
    "    \n",
    "    # set train Data file address\n",
    "    parser1.add_argument('-d', '--TrainPath', action='store',\n",
    "                        default=Var.MainDir+'\\MusicHitTrainData.csv',\n",
    "                        dest='TrainDataPath',help='Enter .csv file address')\n",
    "    \n",
    "    # set test data file address\n",
    "    parser1.add_argument('-t', '--TestPath', action='store',\n",
    "                        default=Var.MainDir+'\\MusicHitTestData.csv',\n",
    "                        dest='TestDataPath',help='Enter .csv file address')\n",
    "    \n",
    "    # set work directory\n",
    "    parser1.add_argument('-w', '--wDirAd', action='store',\n",
    "                        default=Var.MainDir,\n",
    "                        dest='WorkDir',help='Enter work Directory')\n",
    "    \n",
    "    # whether to save plots or not\n",
    "    parser1.add_argument('-p', '--SavePlot', action='store', \n",
    "                         default='1',  dest='SavePlot', choices=['0', '1'],\n",
    "                         help='0: Dont Save plots     1: Save plots')\n",
    "    \n",
    "    # whether to create log file or not\n",
    "    parser1.add_argument('-l', '--log', action='store',\n",
    "                         default='1', dest='logFile', choices=['0', '1'],\n",
    "                         help='0: Dont write logfile     1: write logfile')\n",
    "    \n",
    "    # whether to create report.docx file or not\n",
    "    parser1.add_argument('-r', '--report', action='store',\n",
    "                         default='1', dest='ReportFile', choices=['0', '1'],\n",
    "            help='0: Dont write report file     1: write report file')\n",
    "    \n",
    "    \n",
    "    # indicates when to exit while loop\n",
    "    entry=False\n",
    "    while entry==False:\n",
    "        # initialize\n",
    "        ParsErr=0\n",
    "        FileErr=0\n",
    "        makedirErr=0\n",
    "        \n",
    "        # --------------in this section we try to parse successfully-----------\n",
    "        # function to call input data from command line    \n",
    "        Func.Input_Data_Message()\n",
    "        \n",
    "        # user wanted to continue with default values\n",
    "        if Var.str_input=='':\n",
    "            Var.args=parser1.parse_args()\n",
    "            # exit while loop\n",
    "            entry=True\n",
    "        elif Var.str_input.upper()=='Q':\n",
    "            # exit script\n",
    "            sys.exit()\n",
    "        else:\n",
    "            entry=True\n",
    "            ParsErr=0\n",
    "            try:\n",
    "                Var.args=parser1.parse_args(Var.str_input.split(' '))\n",
    "            except:\n",
    "                entry=False\n",
    "                ParsErr=1\n",
    "        #----------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        #-------------After having parsed successfully, we coninue-------------\n",
    "        # continue if parse was done successfully\n",
    "        if ParsErr==0:  \n",
    "            #check if train data base file exists\n",
    "            TrainFileErr=0\n",
    "            if os.path.isfile(Var.args.TrainDataPath):\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Train Data file address doesn't exist.\")\n",
    "                print('Enter a valid file address.')\n",
    "                entry=False\n",
    "                TrainFileErr=1\n",
    "                \n",
    "            # continue if train data file exists\n",
    "            if TrainFileErr==0:  \n",
    "                #check if test data base file exists\n",
    "                TestFileErr=0\n",
    "                if os.path.isfile(Var.args.TestDataPath):\n",
    "                    pass\n",
    "                else:\n",
    "                    print(\"Test Data file address doesn't exist.\")\n",
    "                    print('Enter a valid file address.')\n",
    "                    entry=False\n",
    "                    TestFileErr=1\n",
    "            \n",
    "                # continue if test file address is correct\n",
    "                if TestFileErr==0:\n",
    "                    #check for work dir. if not exist, create it\n",
    "                    if os.path.exists(Var.args.WorkDir):\n",
    "                        pass\n",
    "                    else:\n",
    "                        makedirErr=0\n",
    "                        try:\n",
    "                            # make work dir \n",
    "                            os.mkdir(Var.args.WorkDir)\n",
    "                        except OSError:  \n",
    "                            print(\"!!!**Work Dir doesn't exist and couldn't be created**!!!\")\n",
    "                            print(\"Try another directory again\")\n",
    "                            entry=False\n",
    "                            makedirErr=1\n",
    "                        except:  \n",
    "                            print(\"!!!**Work Dir doesn't exist and couldn't be created**!!!\")\n",
    "                            print(\"Try another directory again\")\n",
    "                            entry=False\n",
    "                            makedirErr=1\n",
    "                        \n",
    "                        if makedirErr==0:   \n",
    "                            os.chdir(Var.args.WorkDir)\n",
    "                        \n",
    "                        # figs must be saved for report file\n",
    "                        if Var.args.ReportFile==1:\n",
    "                            Var.args.SavePlot=1\n",
    "\n",
    "        #----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function creates and sets result directory\n",
    "#==============================================================================   \n",
    "def CreateAndSetResultsDir():\n",
    "    global path\n",
    "    \n",
    "    # the work directory is set in arg-parse\n",
    "    # get current path\n",
    "    path=Var.args.WorkDir \n",
    "    \n",
    "    # creating results directory and avoid to override the previous results\n",
    "    i=1\n",
    "    ResFldExst=True\n",
    "    while ResFldExst==True:\n",
    "        resultspath = path+\"\\\\Results_Run_\"+str(i)\n",
    "        ResFldExst=os.path.exists(resultspath)\n",
    "        i+=1\n",
    "    Var.resultssubpath=resultspath\n",
    "    # no we create results sub folder and set it as result path\n",
    "    os.mkdir(Var.resultssubpath)\n",
    "    os.chdir(Var.resultssubpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function inputs the parameters for Part A of problem \n",
    "#==============================================================================\n",
    "def CreateSubPlotStructure_PartA(x):\n",
    "    global axes,cmap,xMesh,yMesh,fig_SubPlotC\n",
    "    \n",
    "    nrows=len(Var.C_values)//3  if len(Var.C_values)%3==0 else len(Var.C_values)//3+1\n",
    "    fig_SubPlotC,axes=plt.subplots(nrows=nrows,ncols=3,figsize=(15,5.0*nrows))\n",
    "    cmap=ListedColormap(['#b30065','#178000'])\n",
    "    \n",
    "    xMin,xMax=x[:,0].min()-1,x[:,0].max()+1\n",
    "    yMin,yMax=x[:,1].min()-1,x[:,1].max()+1\n",
    "    xMesh,yMesh=np.meshgrid(np.arange(xMin,xMax,0.01),np.arange(yMin,yMax,0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function sets the classifier parameters of problem\n",
    "#==============================================================================\n",
    "def ClassifierParameters():\n",
    "    \n",
    "    # public parameters\n",
    "    Var.OptVals= {'RF':20,'KNN':13,'SVM':0.1,'LR':1}\n",
    "    Var.Clf_Names=['RF','KNN','SVM','LR']\n",
    "    Var.random_state=np.random.randint(low = 0, high = 100)\n",
    "    Var.tFrac=0.2\n",
    "    Var.cvNum=5\n",
    "    Var.figdpi=100\n",
    "    Color_List=['orange', 'g', 'r', 'c', 'm', 'y', 'k','yellow'\n",
    "                          ,'cyan','b','g', 'r', 'c', 'm','yellow',\n",
    "                          'cyan','orange', 'g',\"darkorange\"]\n",
    "    Var.Color_List=Color_List+Color_List\n",
    "    Var.Time_List=list()\n",
    "    \n",
    "    # Rand Forest parameters\n",
    "    Var.RF_VarPar=[1,2,3,4,5,7,9,11,13,15,20,30,40,50]\n",
    "    # Var.RF_VarPar=[1,2,13,15,20,30]\n",
    "\n",
    "    # Rand Forest criterion\n",
    "    Var.RF_criterion='entropy'\n",
    "    \n",
    "    # KNN parameters\n",
    "    Var.KNN_VarPar=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,20,25]\n",
    "    # Var.KNN_VarPar=[1,2,10,11,12,13,14,15]\n",
    "    Var.KNN_metric='minkowski'\n",
    "    Var.KNN_p=2\n",
    "    \n",
    "    # SVM\n",
    "    Var.SVM_kernel='rbf'\n",
    "    Var.SVM_C=1\n",
    "    Var.SVM_logVarPar=np.arange(-3.5, 1, 0.25)\n",
    "    # Var.SVM_logVarPar=np.arange(-1, 1.5, 0.5)\n",
    "    Var.SVM_VarPar=np.power(10.0, Var.SVM_logVarPar)\n",
    "    \n",
    "    #Logestic regression\n",
    "    Var.LR_logVarPar=np.arange(-4, 2, 0.5)\n",
    "    # Var.LR_logVarPar=np.arange(-1, 2, 0.5)\n",
    "    Var.LR_VarPar=np.power(10.0, Var.LR_logVarPar)\n",
    "    Var.LR_solver='lbfgs'\n",
    "    Var.LR_max_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function inputs the parameters for Part B of problem \n",
    "#==============================================================================\n",
    "def CreateSubPlotStructure_PartB(x):\n",
    "    global axes,cmap,xMesh,yMesh,fig_SubPlotGamma\n",
    "    \n",
    "    nrows=len(Var.GAMMA_values)//3  if len(Var.GAMMA_values)%3==0 else len(Var.GAMMA_values)//3+1\n",
    "    fig_SubPlotGamma,axes=plt.subplots(nrows=nrows,ncols=3,figsize=(15,5.0*nrows))\n",
    "    cmap=ListedColormap(['#b30065','#178000'])\n",
    "    \n",
    "    xMin,xMax=x[:,0].min()-1,x[:,0].max()+1\n",
    "    yMin,yMax=x[:,1].min()-1,x[:,1].max()+1\n",
    "    xMesh,yMesh=np.meshgrid(np.arange(xMin,xMax,0.01),np.arange(yMin,yMax,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function calculate feature importance using ExtraTreesClassifier \n",
    "#==============================================================================\n",
    "def Feature_importance_ExtraTrees():\n",
    "    \n",
    "    n_estimators=[1,3,5,10,15,20,50]\n",
    "    importance_list=list()\n",
    "    \n",
    "    Var.ExtraTrees_importance_list=list()\n",
    "    \n",
    "    for N in n_estimators:\n",
    "        # Build a forest and compute the feature importances\n",
    "        forest = ExtraTreesClassifier(n_estimators=N, \n",
    "                                      random_state=Var.random_state)\n",
    "        \n",
    "        forest.fit(Var.X_trn_scaled, Var.y_trn)\n",
    "        \n",
    "        importances = forest.feature_importances_\n",
    "        importance_list.append([np.arange(Var.X_trn_scaled.shape[1]),\n",
    "                                importances])\n",
    "    \n",
    "        \n",
    "        std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                     axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Plot bar the feature importances of the forest\n",
    "        if N==n_estimators[-1]:\n",
    "            Feature_Impo_BarPlot=plt.figure()\n",
    "            plt.title(\"Feature importance by ExtraTrees\")\n",
    "            plt.bar(range(Var.X_trn_scaled.shape[1]), importances[indices],\n",
    "                   color='yellow', yerr=std[indices], align=\"center\")\n",
    "            # plt.xticks(range(X_trn_scaled.shape[1]), indices)\n",
    "            plt.xticks(range(Var.X_trn_scaled.shape[1]), \n",
    "                       [t for t in Var.df_trn_drp.columns[indices]],\n",
    "                           rotation=80)\n",
    "            plt.xlim([-1, Var.X_trn_scaled.shape[1]])\n",
    "            plt.ylabel('Importance Score')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    Feature_Impo_Plot=plt.figure()\n",
    "   \n",
    "    for i in range(len(n_estimators)):\n",
    "        feature_ID=importance_list[i][0]+1\n",
    "        plt.plot(feature_ID, importance_list[i][1],'--',\n",
    "           color=Var.Color_List[i],\n",
    "           label='n={}'.format(n_estimators[i]))\n",
    "    \n",
    "    plt.title(\"Feature importances by ExtraTrees\")\n",
    "    plt.xticks(range( Var.X_trn_scaled.shape[1]+1))\n",
    "    plt.xlim([0, Var.X_trn_scaled.shape[1]+1])\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Feature No.')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.show()\n",
    "    \n",
    "    if Var.saveplotFlag==True:\n",
    "        Feature_Impo_BarPlot.savefig('Imp_1.jpg', \n",
    "                     dpi=Var.figdpi, facecolor='w')            \n",
    "        Feature_Impo_Plot.savefig('Imp_2.jpg', \n",
    "                     dpi=Var.figdpi, facecolor='w', edgecolor='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function calculate feature importance using PCA \n",
    "#==============================================================================\n",
    "def Feature_importance_PCA():\n",
    "    \n",
    "    Feat_Names = list(Var.df_trn_drp.keys())\n",
    "    # PCA analysis\n",
    "    n_Features=len(Var.df_trn_drp.columns)\n",
    "    # number of components kept\n",
    "    n_pcs= n_Features\n",
    "    pca = PCA(n_components=n_pcs)\n",
    "    pca.fit_transform(Var.X_trn_scaled)\n",
    "    \n",
    "#    X_trn_scaled=pca.fit_transform(Var.X_trn_scaled)\n",
    "#    X_tst_scaled=pca.fit_transform(X_tst_scaled)\n",
    "    \n",
    "    \n",
    "    print('pca.components_',pca.components_,end='\\n\\n')\n",
    "    print('pca.explained_variance_',pca.explained_variance_,end='\\n\\n')\n",
    "    var_ratio=pca.explained_variance_ratio_\n",
    "    print('var_ratio   ',var_ratio,end='\\n\\n')\n",
    "    #print('pca.singular_values_',pca.singular_values_,end='\\n\\n')\n",
    "\n",
    "    # get most important feature names\n",
    "    print('n_pcs',n_pcs,end='\\n\\n')\n",
    "    percent=var_ratio[0:n_pcs].sum()\n",
    "    print('percent ',percent*100,end='\\n\\n')\n",
    "    \n",
    "    # get the index of the most important feature on EACH component i.e. largest \n",
    "    #absolute value using LIST COMPREHENSION HERE\n",
    "    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "    print('most_important',most_important,end='\\n\\n')\n",
    "    \n",
    "    # get the names\n",
    "    most_important_names = [Feat_Names[most_important[i]] for i in range(n_pcs)]\n",
    "    print('most_important_names',most_important_names,end='\\n\\n')\n",
    "    \n",
    "    # using LIST COMPREHENSION HERE AGAIN\n",
    "    dic = {'PC{:2.0f}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "    print('dic',dic,end='\\n\\n')\n",
    "    \n",
    "    # build the dataframe\n",
    "    df_most_important = pd.DataFrame(sorted(dic.items()))\n",
    "    \n",
    "    print('df_most_important',df_most_important,end='\\n\\n')\n",
    "   \n",
    "    \n",
    "    # ---------------------plot variance ratio---------------------------------\n",
    "    x=np.arange(len(pca.explained_variance_))+1\n",
    "    width=0.5\n",
    "    \n",
    "    fig_PCA_var, ax = plt.subplots()\n",
    "    fig1=plt.gca()\n",
    "    \n",
    "    rects1=plt.bar(x, pca.explained_variance_ratio_, color='y',width = width)\n",
    "    \n",
    "    var_pct=list(pca.explained_variance_ratio_*100)\n",
    "    \n",
    "    def autolabel(rects):\n",
    "        # Attach a text label above each bar in *rects*, displaying its height\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            plt.annotate('{:.0f}%'.format(height*100),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    autolabel(rects1)\n",
    "    \n",
    "    plt.title('PCA (variance ratio)')\n",
    "    plt.ylabel('Score')\n",
    "    fig1.set_xticks(x)\n",
    "    fig1.set_xticklabels(most_important_names,  rotation=80)\n",
    "    plt.show()\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    " \n",
    "    # -------------------------- plot variance --------------------------------\n",
    "    fig_PCA_varRatio=plt.figure()\n",
    "    fig1=plt.gca()\n",
    "    \n",
    "    rects2=plt.bar(x, pca.explained_variance_, color='c', width = width)\n",
    "    \n",
    "    plt.title(\"PCA ( variance )\")\n",
    "    plt.ylabel('Score')\n",
    "    fig1.set_xticks(x)\n",
    "    fig1.set_xticklabels(most_important_names,  rotation=80)\n",
    "    plt.show()\n",
    "    \n",
    "    if Var.saveplotFlag==True:\n",
    "        fig_PCA_var.savefig('PCA_1.jpg',\n",
    "                            dpi=Var.figdpi, facecolor='w')            \n",
    "        fig_PCA_varRatio.savefig('PCA_2.jpg',\n",
    "                        dpi=Var.figdpi, facecolor='w', edgecolor='r')\n",
    "    #--------------------------------------------------------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function reduces features using PCA\n",
    "#==============================================================================\n",
    "def Feature_reduction_PCA():\n",
    "\n",
    "    # number of components kept\n",
    "    n_pcs= 10\n",
    "    pca = PCA(n_components=n_pcs)\n",
    "    \n",
    "    Var.X_trn_scaled=pca.fit_transform(Var.X_trn_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function gets computer spec\n",
    "#==============================================================================\n",
    "def GetSysInfo():\n",
    "    import platform,socket,re,uuid,psutil\n",
    "    try:\n",
    "        Var.sysinfo.append(['platform',platform.system()]) \n",
    "        Var.sysinfo.append(['platform-release',platform.release()])\n",
    "        Var.sysinfo.append(['platform-version',platform.version()])\n",
    "        Var.sysinfo.append(['architecture',platform.machine()])\n",
    "        Var.sysinfo.append(['hostname',socket.gethostname()])\n",
    "        Var.sysinfo.append(['ip-address',socket.gethostbyname(socket.gethostname())])\n",
    "        Var.sysinfo.append(['mac-address',':'.join(re.findall('..', '%012x' % uuid.getnode()))])\n",
    "        Var.sysinfo.append(['processor',platform.processor()])\n",
    "        Var.sysinfo.append(['ram',str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function plots train and validation errors of PartA\n",
    "#==============================================================================\n",
    "def PltTrnValErr_PartA():\n",
    "    global fig_TrnVal_PartA\n",
    "    \n",
    "    # initializethe lists\n",
    "    trnErrList_A=[]\n",
    "    valErrList_A=[]\n",
    "    \n",
    "    # extracting data from Dict to List\n",
    "    fig_TrnVal_PartA=plt.figure(80)\n",
    "    for i,(xx,yy) in enumerate(Var.trnErr_A.items()):\n",
    "        trnErrList_A.append([xx,yy])\n",
    "        \n",
    "    for i,(xx,yy) in enumerate(Var.valErr_A.items()):\n",
    "        valErrList_A.append([xx,yy])\n",
    "        \n",
    "    # we need to sort because they are added to the list as soon as they finish\n",
    "    # using sort() + lambda to sort list of list (sort by second index) \n",
    "    trnErrList_A.sort(key=lambda trnErrList_A:trnErrList_A[0]) \n",
    "    valErrList_A.sort(key=lambda valErrList_A:valErrList_A[0]) \n",
    "    \n",
    "    Var.trnErrArray_A=np.array(trnErrList_A)\n",
    "    Var.valErrArray_A=np.array(valErrList_A)\n",
    "\n",
    "    #plotting train validation plot\n",
    "    plt.plot(np.log10(Var.trnErrArray_A[:,0]),Var.trnErrArray_A[:,1],'*',color='black')\n",
    "    plt.plot(np.log10(Var.valErrArray_A[:,0]),Var.valErrArray_A[:,1],'*',color='black') \n",
    "    \n",
    "    plt.plot(np.log10(Var.trnErrArray_A[:,0]),Var.trnErrArray_A[:,1],'-',color='blue', label='train') \n",
    "    plt.plot(np.log10(Var.valErrArray_A[:,0]),Var.valErrArray_A[:,1],'-',color='red', label='validation')\n",
    "    \n",
    "    plt.legend(loc='lower right',frameon=False)\n",
    "    plt.xlabel('log(C values)') \n",
    "    plt.ylabel('Accuracy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function plots train and validation errors of Part B\n",
    "#==============================================================================\n",
    "def PltTrnValErr_PartB():\n",
    "    global fig_TrnVal_PartB\n",
    "    \n",
    "    # initializethe lists\n",
    "    trnErrList_B=[]\n",
    "    valErrList_B=[]\n",
    "    \n",
    "    # extracting data from Dict to List\n",
    "    fig_TrnVal_PartB=plt.figure(50)\n",
    "    for i,(xx,yy) in enumerate(Var.trnErr_B.items()):\n",
    "        trnErrList_B.append([xx,yy])\n",
    "        \n",
    "    for i,(xx,yy) in enumerate(Var.valErr_B.items()):\n",
    "        valErrList_B.append([xx,yy])\n",
    "        \n",
    "    # we need to sort because they are added to the list as soon as they finish\n",
    "    # using sort() + lambda to sort list of list (sort by second index) \n",
    "    trnErrList_B.sort(key=lambda trnErrList_B:trnErrList_B[0]) \n",
    "    valErrList_B.sort(key=lambda valErrList_B:valErrList_B[0]) \n",
    "    \n",
    "    Var.trnErrArray_B=np.array(trnErrList_B)\n",
    "    Var.valErrArray_B=np.array(valErrList_B)\n",
    "\n",
    "    #plotting train validation plot\n",
    "    plt.plot(np.log10(Var.trnErrArray_B[:,0]),Var.trnErrArray_B[:,1],'*',color='black')\n",
    "    plt.plot(np.log10(Var.valErrArray_B[:,0]),Var.valErrArray_B[:,1],'*',color='black') \n",
    "    \n",
    "    plt.plot(np.log10(Var.trnErrArray_B[:,0]),Var.trnErrArray_B[:,1],'-',color='blue', label='train') \n",
    "    plt.plot(np.log10(Var.valErrArray_B[:,0]),Var.valErrArray_B[:,1],'-',color='red', label='validation')\n",
    "    \n",
    "    plt.legend(loc='lower right',frameon=False)\n",
    "    plt.xlabel('log(Gamma values)') \n",
    "    plt.ylabel('Accuracy')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function prinrtsparsed data\n",
    "#==============================================================================\n",
    "def PrintParsedData(): \n",
    "    print('') \n",
    "    print('  ========================Parsed  Data=====================')  \n",
    "    print('  ', Var.args)\n",
    "    print('')\n",
    "    print('  Script Path                =', Var.args.ScriptPath)\n",
    "    print('  .csv Train file address    =', Var.args.TrainDataPath)\n",
    "    print('  .csv Test file address     =', Var.args.TestDataPath)\n",
    "    print('  Work Directory             =', Var.args.WorkDir)\n",
    "    # results path is ceated ans set in CreateAndSetResultsDir()\n",
    "    print('  Results Path               =', os.getcwd())\n",
    "    print('  Save Plots                 =', Var.args.SavePlot)\n",
    "    print('  Create Log File            =', Var.args.logFile)\n",
    "    print('  Create Report File         =', Var.args.ReportFile)\n",
    "    print('  =========================================================')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function reads and splits data\n",
    "#==============================================================================        \n",
    "def Read_Scale_Data(trnPath,tstPath):        \n",
    "    # read data\n",
    "    Var.df_trn=pd.read_csv(trnPath)\n",
    "    Var.df_tst=pd.read_csv(tstPath)\n",
    "    print(Var.df_trn.head())\n",
    "    # drop features\n",
    "    Var.df_trn_drp = Var.df_trn.drop(columns=['Artist','Track','Year','Label'])\n",
    "    Var.y_trn=Var.df_trn['Label']\n",
    "\n",
    "    # StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    Var.X_trn_scaled=scaler.fit_transform(Var.df_trn_drp)\n",
    "    Var.n_Features=len(Var.df_trn_drp.columns)\n",
    "    \n",
    "    # preparing test data for prediction\n",
    "    Var.df_tst_dropped = Var.df_tst.drop(columns=['Valence','Acousticness'])\n",
    "    Var.tst_dropped=Var.df_tst_dropped.values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    Var.tst_data_dropped_scaled=scaler.fit_transform(Var.tst_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function saves results of predcrions of test data into csv file\n",
    "#==============================================================================        \n",
    "def SavePredResults_CVSFile():\n",
    "    Rasults=np.hstack((Var.RF.predicted_data,Var.KNN.predicted_data,\n",
    "                   Var.SVM.predicted_data, Var.LR.predicted_data))\n",
    "    df_results=pd.DataFrame(Rasults,columns=Var.Clf_Names)\n",
    "    df_results.to_csv('Results.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function sets flags for log and save plot\n",
    "#==============================================================================        \n",
    "def SetFlags():        \n",
    "\n",
    "    Var.saveplotFlag=False if Var.args.SavePlot=='0' else True\n",
    "    Var.logFlag=False if Var.args.logFile=='0' else True\n",
    "    Var.docxFlag=False if Var.args.ReportFile=='0' else True\n",
    "    \n",
    "    if Var.docxFlag==True: Var.logFlag=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function splits data\n",
    "#==============================================================================        \n",
    "def Split(): \n",
    "    Var.X_trn,Var.X_tst,Var.y_trn,Var.y_tst=train_test_split(\n",
    "            Var.X_trn_scaled,Var.y_trn,\n",
    "              test_size=Var.tFrac,random_state=Var.random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function does the calculation of prediction and prepare plot data Part A\n",
    "#==============================================================================\n",
    "def visualize_PartA(p,clf,param,x,y,index):\n",
    "    \n",
    "    # train the model\n",
    "    clf_trained=clf.fit(x,y)\n",
    "    \n",
    "    # we save the trained models to be used later in cross val\n",
    "    Var.SVCTrainedModelsDict_C[p]=clf_trained\n",
    "    \n",
    "    # define which plot to be used for this svc model\n",
    "    r,c=np.divmod(index,3)\n",
    "    ax=axes[r,c]\n",
    "    print(\"index,r,c\",index,r,c)\n",
    "\n",
    "    # Plot contours\n",
    "    zMesh=clf.decision_function(np.c_[xMesh.ravel(),yMesh.ravel()])\n",
    "    zMesh=zMesh.reshape(xMesh.shape)\n",
    "    ax.contourf(xMesh,yMesh,zMesh,cmap=plt.cm.PiYG,alpha=0.6)\n",
    "    \n",
    "    ax.contour(xMesh,yMesh,zMesh,colors='k',levels=[-1,0,1],\n",
    "               alpha=0.5,linestyles=['--','-','--'])\n",
    "\n",
    "    # Plot data\n",
    "    ax.scatter(x[:,0],x[:,1],c=y,cmap=cmap,edgecolors='k')\n",
    "    ax.set_title('{0}={1}'.format(param, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# this function does the calculation of prediction and prepare plot data Part B\n",
    "#==============================================================================\n",
    "def visualize_PartB(p,clf,param,x,y,index):\n",
    "    \n",
    "    # train the model\n",
    "    clf_trained=clf.fit(x,y)\n",
    "    \n",
    "    # we save the trained models to be used later in cross val\n",
    "    Var.SVCTrainedModelsDict_Gamma[p]=clf_trained\n",
    "    \n",
    "    # define which plot to be used for this svc model\n",
    "    r,c=np.divmod(index,3)\n",
    "    ax=axes[r,c]\n",
    "    print(\"index,r,c\",index,r,c)\n",
    "\n",
    "    # Plot contours\n",
    "    zMesh=clf.decision_function(np.c_[xMesh.ravel(),yMesh.ravel()])\n",
    "    zMesh=zMesh.reshape(xMesh.shape)\n",
    "    ax.contourf(xMesh,yMesh,zMesh,cmap=plt.cm.PiYG,alpha=0.6)\n",
    "    \n",
    "    ax.contour(xMesh,yMesh,zMesh,colors='k',levels=[-1,0,1],\n",
    "               alpha=0.5,linestyles=['--','-','--'])\n",
    "\n",
    "    # Plot data\n",
    "    ax.scatter(x[:,0],x[:,1],c=y,cmap=cmap,edgecolors='k')\n",
    "    ax.set_title('{0}={1}'.format(param, p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
